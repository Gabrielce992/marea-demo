
# ğŸ¯ Proyecto: **MAREA-UI â€” Interfaz Multimodal de AnÃ¡lisis de Emociones en Tiempo Real**

**Tipo de archivo:** Python Â· TensorFlow/Keras Â· HuggingFace Â· Librosa Â· OpenCV
**Frameworks:** TensorFlow/Keras 2.19.0 Â· HuggingFace Transformers 4.55.4 Â· Librosa/soundfile Â· OpenCV/PIL

---

## ğŸ“‘ DescripciÃ³n

**MAREA-UI** es una **interfaz demostrativa interactiva** que permite analizar **emociones en tiempo real (Positivo / Negativo)** a partir de entradas de **texto, audio e imagen**.

ğŸ” Aunque el modelo multimodal subyacente forma parte de un tema de **investigaciÃ³n acadÃ©mica** y su cÃ³digo **no se expone pÃºblicamente**, esta interfaz fue desarrollada como un **aporte propio**, enfocÃ¡ndose en la usabilidad, accesibilidad y la visualizaciÃ³n de resultados.

El usuario puede:

* âœï¸ Escribir un texto.
* ğŸ¤ Grabar o subir un audio.
* ğŸ–¼ï¸ Subir una imagen (rostro).
* Usar una, dos o las tres modalidades en cualquier combinaciÃ³n.

âš ï¸ **Aviso:** El backend del modelo corresponde a un trabajo acadÃ©mico con acceso restringido.
ğŸ‘‰ Lo destacado en este proyecto es la **interfaz adaptativa y explicativa**.

---

## âš™ï¸ Flujo de Uso â€“ Interfaz DEMO

1. **Ingresa tus Datos**
   Texto, audio y/o imagen.

2. **Analiza la EmociÃ³n**
   Presiona el botÃ³n **Analizar EmociÃ³n**.

3. **Interpreta los Resultados**

   * âœ… PredicciÃ³n global (clase mÃ¡s probable en %).
   * âœ… DecisiÃ³n binaria por umbral.
   * âœ… ContribuciÃ³n relativa por modalidad.
   * âœ… VisualizaciÃ³n de salidas crudas para debugging.

---

## ğŸš€ Ejemplo de Resultados

* **PredicciÃ³n Global:** Negativo (88%)
* **Umbral positivo (0.50)** â†’ Resultado: Negativo
* **Fuente predominante:** Clasificador de Imagen
* **Modalidades usadas:** Texto âŒ | Audio âŒ | Imagen âœ…

---

## ğŸ¬ Demo en AcciÃ³n

* ğŸ“¹ **GIF de Ejemplo de la Interfaz en uso**
  ![GIF Demo](demoma.gif)

* ğŸ“¸ **Capturas de Pantalla**
  ![Screenshot 1](Captusrsa.JPG)
 
---

## ğŸ”§ CaracterÃ­sticas Clave de la Interfaz

* ğŸ§  **IntegraciÃ³n multimodal**: conecta texto, audio e imagen en un solo flujo visual.
* ğŸ”„ **Adaptativa en tiempo real**: ajusta dinÃ¡micamente la importancia de cada modalidad segÃºn disponibilidad.
* ğŸ“Š **Explicabilidad clara**: el usuario puede ver quÃ© modalidad pesÃ³ mÃ¡s en la decisiÃ³n.
* ğŸ¨ **DiseÃ±o interactivo y accesible**: pensado para uso acadÃ©mico y demostrativo.

---

## ğŸ“š Contexto AcadÃ©mico

* El motor multimodal subyacente se entrenÃ³ con:

  * ğŸµ **RAVDESS** â€” audios emocionales (\~2,880 muestras).
  * ğŸ™‚ **AffectNet** â€” imÃ¡genes faciales etiquetadas.
  * ğŸ’¬ **Reddit** â€” comentarios positivos/negativos.

ğŸ“Œ **Nota:** Estos datasets forman parte del marco acadÃ©mico; **el cÃ³digo del modelo no es pÃºblico**.

---

## ğŸ“¦ Estado del Proyecto

âœ… **Prototipo funcional con interfaz demostrativa.**
ğŸš« **CÃ³digo fuente y modelos entrenados accesibles solo bajo permiso.**

---

## ğŸ‘¤ Autor & Contacto

**Autor:** Gabriel Angel CastaÃ±eda Huaytalla
ğŸ“§ **Correo:** [gabrielce992@gmail.com](mailto:gabrielce992@gmail.com)
ğŸ“± **Celular / WhatsApp:** +51 991 744 486 ğŸ‡µğŸ‡ª

